{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75126338-1de6-4556-aa2f-4eff2c627464",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Preparation\n",
    "\n",
    "**1.0 Import Lexicons** <br>\n",
    "Initially we intended to use LIWC lexicon dictionairies (download [here](https://pypi.org/project/liwc/), and install using `!pip install -U liwc`). But it would require a substantial fee. So we turned to a free alternative called EMPATH (guideline could be accessed [here](https://github.com/Ejhfast/empath-client).) If this does not perform adequately, we may explore the [SEANCE](https://www.linguisticanalysistools.org/seance.html). as a secondary option. <br>\n",
    "\n",
    "**1.1 Explore EMPATHY, identifying relevant existing lexicons for direct adoption.** <br>\n",
    "We then examined the EMPATHY. In [Yarkoni (2011)](https://www.sciencedirect.com/science/article/pii/S0092656610000541), Table 1 outlines the correlation between LIWC lexicons and the Big-Five personality dimentions. Using this as a benchmark we filter through the EMPATHY, identified relevant labels, and applied them to our dataset. <br>\n",
    "\n",
    "**1.2 Use Spacy to add lexicons that we need but missing from EMPATHY.** <br>\n",
    "For those lexicons missing from EMPATHY, such as 1st, 2nd, 3rd person pronouns, we applied Spacy for parsing. But certain categories remain inaccessible. We will acknowledge that as a limitation of this study. <br> \n",
    "\n",
    "**The results shows that:**\n",
    "\n",
    "- EMPATHY includes: \n",
    "\n",
    "`affect`, `positive_emotions`, `negative_emotions`, `anger`, `sadness`, `hearing`, `communication`, `friends`, `family`, `swearing_terms`\n",
    "\n",
    "- Require Spacy: \n",
    "\n",
    "`pronouns(PRON)`, `articles(DET)`, `prepositions(PREP)`, `numbers(NUM)`,\n",
    "`1st person sg/pl`, `2nd person`, `3rd person pronouns`,\n",
    "`Past/present/future tense vb`.\n",
    "\n",
    "The remaining categories are then excluded from this study.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0822a4dd-ab01-4254-8101-04dd13574065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['help', 'office', 'dance', 'money', 'wedding', 'domestic_work', 'sleep', 'medical_emergency', 'cold', 'hate', 'cheerfulness', 'aggression', 'occupation', 'envy', 'anticipation', 'family', 'vacation', 'crime', 'attractive', 'masculine', 'prison', 'health', 'pride', 'dispute', 'nervousness', 'government', 'weakness', 'horror', 'swearing_terms', 'leisure', 'suffering', 'royalty', 'wealthy', 'tourism', 'furniture', 'school', 'magic', 'beach', 'journalism', 'morning', 'banking', 'social_media', 'exercise', 'night', 'kill', 'blue_collar_job', 'art', 'ridicule', 'play', 'computer', 'college', 'optimism', 'stealing', 'real_estate', 'home', 'divine', 'sexual', 'fear', 'irritability', 'superhero', 'business', 'driving', 'pet', 'childish', 'cooking', 'exasperation', 'religion', 'hipster', 'internet', 'surprise', 'reading', 'worship', 'leader', 'independence', 'movement', 'body', 'noise', 'eating', 'medieval', 'zest', 'confusion', 'water', 'sports', 'death', 'healing', 'legend', 'heroic', 'celebration', 'restaurant', 'violence', 'programming', 'dominant_heirarchical', 'military', 'neglect', 'swimming', 'exotic', 'love', 'hiking', 'communication', 'hearing', 'order', 'sympathy', 'hygiene', 'weather', 'anonymity', 'trust', 'ancient', 'deception', 'fabric', 'air_travel', 'fight', 'dominant_personality', 'music', 'vehicle', 'politeness', 'toy', 'farming', 'meeting', 'war', 'speaking', 'listen', 'urban', 'shopping', 'disgust', 'fire', 'tool', 'phone', 'gain', 'sound', 'injury', 'sailing', 'rage', 'science', 'work', 'appearance', 'valuable', 'warmth', 'youth', 'sadness', 'fun', 'emotional', 'joy', 'affection', 'traveling', 'fashion', 'ugliness', 'lust', 'shame', 'torment', 'economics', 'anger', 'politics', 'ship', 'clothing', 'car', 'strength', 'technology', 'breaking', 'shape_and_size', 'power', 'white_collar_job', 'animal', 'party', 'terrorism', 'smell', 'disappointment', 'poor', 'plant', 'pain', 'beauty', 'timidity', 'philosophy', 'negotiate', 'negative_emotion', 'cleaning', 'messaging', 'competing', 'law', 'friends', 'payment', 'achievement', 'alcohol', 'liquid', 'feminine', 'weapon', 'children', 'monster', 'ocean', 'giving', 'contentment', 'writing', 'rural', 'positive_emotion', 'musical']\n",
      "\n",
      "Matched categories: ['money', 'domestic_work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school', 'social_media', 'blue_collar_job', 'optimism', 'home', 'sexual', 'superhero', 'religion', 'body', 'eating', 'sports', 'death', 'communication', 'hearing', 'weather', 'music', 'sound', 'work', 'sadness', 'emotional', 'affection', 'anger', 'white_collar_job', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'musical']\n",
      "\n",
      "Not matched categories: ['total pronouns', 'pron', 'first person sing.', 'first person', 'second person', 'third person', 'negation', 'assent', 'prep', 'prepositions', 'number', 'affect', 'positive', 'negative', 'anxiety', 'cognitive', 'causation', 'insight', 'discrepancy', 'inhibition', 'tentative', 'certainty', 'sensory', 'seeing', 'feeling', 'social', 'references', 'friend', 'human', 'time', 'tense', 'space', 'up', 'down', 'inclusive', 'exclusive', 'motion', 'job', 'achieve', 'sport', 'tv', 'movie', 'finance', 'metaphysics', 'physical', 'sex', 'eat', 'drink', 'groom', 'swear']\n"
     ]
    }
   ],
   "source": [
    "### 1.0 Import Lexicon pkg ###\n",
    "##############################\n",
    "\n",
    "# !pip install empath spacy pandas numpy scipy statsmodels\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from empath import Empath\n",
    "lexicon = Empath()\n",
    "\n",
    "### 1.1 EXPLORING EMPATHY ###\n",
    "### WHAT EXISTING CLASSES IN EMPATHY COULD BE ADOPTED DIRECTLY ###\n",
    "# Print all category (class) names\n",
    "print(list(lexicon.cats.keys()))\n",
    "print()\n",
    "\n",
    "# Define the list of class (category) names we're looking for\n",
    "categories_to_check = [\"total pronouns\", \"pron\", \"first person sing.\", \"first person\", \"second person\", \"third person\",\n",
    " \"negation\", \"assent\", \"articles\", \"prep\", \"prepositions\", \"number\",\n",
    " \"affect\", \"positive\", \"optimism\", \"negative\", \"anxiety\", \"anger\", \"sadness\", \n",
    " \"cognitive\", \"causation\", \"insight\", \"discrepancy\", \"inhibition\", \"tentative\", \"certainty\", \n",
    " \"sensory\", \"seeing\", \"hearing\", \"feeling\", \"social\", \"communication\", \"references\",\n",
    " \"friend\", \"family\", \"human\", \"time\", \"tense\", \"space\", \"up\", \"down\", \n",
    " \"inclusive\", \"exclusive\", \"motion\", \"occupation\", \"school\", \"job\", \"work\", \"achieve\", \n",
    " \"leisure\", \"home\", \"sport\", \"tv\", \"movie\", \"music\", \"sound\", \"money\", \"finance\",\n",
    " \"metaphysics\", \"religion\", \"death\", \"physical\", \"body\", \"sexuality\", \"sex\", \"eat\", \"drink\", \"sleep\", \"groom\", \"swear\"]\n",
    "\n",
    "# Convert categories_to_check to lowercase for case-insensitive comparison\n",
    "categories_to_check_lower = [cat.lower() for cat in categories_to_check]\n",
    "\n",
    "# Find matching categories (substring match, case insensitive)\n",
    "matched_categories = [cat for cat in lexicon.cats if any(search_term in cat.lower() for search_term in categories_to_check_lower)]\n",
    "not_matched_categories = [cat for cat in categories_to_check if not any(search_term in cat.lower() for search_term in lexicon.cats.keys())]\n",
    "\n",
    "# Output matched and not matched categories\n",
    "print(\"Matched categories:\", matched_categories)\n",
    "print()\n",
    "print(\"Not matched categories:\", not_matched_categories)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00766a35-53ff-43a7-aaff-8ddc02242ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do an example analysis for light testing\n",
    "\n",
    "# result = lexicon.analyze(\"he kiss the other person\", normalize=True)\n",
    "# filtered_result = {category: value for category, value in result.items() if value > 0}\n",
    "# print(filtered_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01f706-de99-4624-82f9-8f5f34227d71",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Data Processing\n",
    "\n",
    "**Recall the hypotheses for word level**\n",
    "\n",
    "We aim to investigate whether correlations between LIWC categories and Big Five personality traits observed in Yarkoni (2011) align with our dataset’s trends. \n",
    "\n",
    "**Hypotheses Summary**\n",
    "\n",
    "|   EMPATHYe   |Label Name| Neuroticism  | Extroversion |   Openness   |Agreeableness |Conscientiousness|\n",
    "|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\n",
    "| pronouns     |*pronouns|      +       |      +       |       --     |       ++     |       -      |\n",
    "| 1st person sing.|*first_person_sg|   ++      |      +       |       -      |       +      |       0      |\n",
    "| 1st person plural|*first_person_pl|   -      |     ++       |       --     |       ++     |       +      |\n",
    "| 1st person   |*first_person||++|+|--|++|+|\n",
    "| 2nd person   |*second_person||--|++|--|+|0|\n",
    "| 3rd person   |*third_person|+|+|-|+|-|\n",
    "| negations    |*negations|++|-|--|-|--|\n",
    "| articles     |*articles|--|-|++|+|++|\n",
    "| prepositions |*prepositions|-|-|++|+|+|\n",
    "| numbers      |*numbers|-|--|--|++|+|\n",
    "| affect       |affection|+|+|--|+|-|\n",
    "| positive emotions|positive_emotion|-|++|--|++|+|\n",
    "| optimism    |optimism|--|+|0|++|++|\n",
    "| negative emotions|negative_emotion|++|+|0|--|--|\n",
    "| anger        |anger|++|+|+|--|--|\n",
    "| sadness      |sadness|++|+|-|+|--|\n",
    "| hearing      |hearing|+|++|--|+|--|\n",
    "| communication|communication|0|++|-|+|-|\n",
    "| friends      |friends|--|++|-|++|+|\n",
    "| family       |family|-|+|--|++|+|\n",
    "| past tense vb.|*past_tense|+|-|--|+|0|\n",
    "| present tense vb.|*present_tense|+|-|--|0|-|\n",
    "| future tense vb.|*future_tense|-|-|-|-|-|\n",
    "| occupation   |occupation|+|--|+|-|+|\n",
    "| school       |school|+|-|+|-|-|\n",
    "| job/work     |work|+|--|+|-|+|\n",
    "| achievement  |achievement|+|--|-|+|--|\n",
    "| leisure      |leisure|-|++|--|++|+|\n",
    "| home         |home|0|+|--|++|+|\n",
    "| sports       |sports|-|+|--|+|0|\n",
    "| music        |music|-|++|+|+|--|\n",
    "| money        |money|+|-|-|--|-|\n",
    "| religion     |religion|-|++|+|+|-|\n",
    "| death        |death|+|+|++|--|--|\n",
    "| body states  |body|+|++|-|++|-|\n",
    "| sexuality    |sexuality|+|++|0|++|-|\n",
    "| eating       |eating|-|+|--|+|-|\n",
    "| sleep        |sleep|++|-|--|++|-|\n",
    "| swearing words|swearing_terms|++|+|+|--|--|\n",
    "(_Label Name_ refers to its new name in our _EMPATHYe_)\n",
    "\n",
    "**Re-classification Rquired**:<br>\n",
    "The following labels from _EMPATHY_ will be reclassified and renamed in our lexicon:\n",
    "- Work: domestick_work, blue_collar_job, white_collar_job, work\n",
    "- Music: music, sound, musical \n",
    "- Sexuality: sexual \n",
    "\n",
    "**Dataset**<br>\n",
    "Our dataset use the complete collection of the eight *Harry Potter* film. Initially, we used only the first film, but this yielded insufficient data for significance, so we included all films in the end. <br>\n",
    "\n",
    "**developing our own lexicon:** <br>\n",
    "After the pre-processing stage,  each character’s dialogue forms an individual dataset labeled with tokens, frequencies, and part-of-speech tags. We develope a new lexicon with labels containing the necessary lexicon data: some labels are processed directly using EMPATH, while others require additional parsing with SpaCy.<br>\n",
    "\n",
    "We proceed as follows:\n",
    "\n",
    "**2.1 Handling Empathy** <br>\n",
    "We apply EMPATH to identify and annotate the lexicons in each character's lines, focusing on existing categories relevant to our study, such as emotional and social words, as outlined in our lexicon guide.\n",
    "\n",
    "**2.2 Applying Spacy** <br>\n",
    "Using SpaCy, we tag and extract lexical categories unavailable in EMPATH, such as specific pronoun types and verb tenses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5300aa92-6806-4c14-8a61-a276644b32cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current labels in lexicon: dict_keys(['money', 'work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school', 'optimism', 'home', 'sexuality', 'religion', 'body', 'eating', 'sports', 'death', 'communication', 'hearing', 'music', 'sadness', 'emotional', 'affection', 'anger', 'negative_emotion', 'friends', 'achievement', 'positive_emotion'])\n"
     ]
    }
   ],
   "source": [
    "### 2.1 START WITH EMPATHY ###\n",
    "##############################\n",
    "\n",
    "# labels to keep\n",
    "labels_to_keep = [\n",
    "    'money', 'domestic_work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school',\n",
    "    'blue_collar_job', 'optimism', 'home', 'sexual', 'religion', 'body', 'eating', 'sports',\n",
    "    'death', 'communication', 'hearing', 'music', 'sound', 'work', 'sadness', 'emotional', 'affection',\n",
    "    'anger', 'white_collar_job', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'musical'\n",
    "]\n",
    "\n",
    "# merging rules:\n",
    "merge_rules = {\n",
    "    'work': ['domestic_work', 'blue_collar_job', 'white_collar_job', 'work'],\n",
    "    'music': ['music', 'sound', 'musical'],\n",
    "    'sexuality': ['sexual']\n",
    "}\n",
    "\n",
    "temp_lexicon = {} # temporarily store the lexicon\n",
    "\n",
    "# filter and merge based on the rules above\n",
    "for label in labels_to_keep:\n",
    "    # 检查标签是否需要合并\n",
    "    merged = False\n",
    "    for new_label, old_labels in merge_rules.items():\n",
    "        if label in old_labels:\n",
    "            # 如果是要合并的标签，将内容合并至新标签\n",
    "            if new_label not in temp_lexicon:\n",
    "                temp_lexicon[new_label] = set()\n",
    "            temp_lexicon[new_label].update(lexicon.cats[label])\n",
    "            merged = True\n",
    "            break\n",
    "    # 如果标签不在合并规则内，直接添加到临时存储中\n",
    "    if not merged:\n",
    "        temp_lexicon[label] = lexicon.cats[label]\n",
    "\n",
    "# Clear original lexicon contents\n",
    "lexicon.cats.clear()\n",
    "\n",
    "# Reassign the updated content to lexicon\n",
    "lexicon.cats.update(temp_lexicon)\n",
    "\n",
    "# Print the current label names in the lexicon\n",
    "print(\"Current labels in lexicon:\", lexicon.cats.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3f80c9c6-a2b8-48cc-a931-007a3a4a354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['money', 'work', 'sleep', 'occupation', 'family', 'swearing_terms', 'leisure', 'school', 'optimism', 'home', 'sexuality', 'religion', 'body', 'eating', 'sports', 'death', 'communication', 'hearing', 'music', 'sadness', 'emotional', 'affection', 'anger', 'negative_emotion', 'friends', 'achievement', 'positive_emotion', 'first_person_sg', 'first_person_pl', 'first_person', 'second_person', 'third_person']\n"
     ]
    }
   ],
   "source": [
    "### 2.2 USE SPACY TO PROCEED MORE ###\n",
    "#####################################\n",
    "\n",
    "# [1] PERSONAL PRONOUNS\n",
    "nlp = spacy.load(\"en_core_web_lg\") # load the English model\n",
    "# 定义代词标签\n",
    "pronouns = {\n",
    "    \"first_person_sg\": [\"I\", \"me\", \"my\", \"mine\"],\n",
    "    \"first_person_pl\": [\"we\", \"us\", \"our\", \"ours\"],\n",
    "    \"first_person\": [\"I\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"],\n",
    "    \"second_person\": [\"you\", \"your\", \"yours\"],\n",
    "    \"third_person\": [\"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\", \"theirs\"],\n",
    "}\n",
    "\n",
    "# 将 pronouns 添加到 lexicon\n",
    "for label, words in pronouns.items():\n",
    "    lexicon.cats[label] = words\n",
    "\n",
    "# 检查添加后的 lexicon\n",
    "print(list(lexicon.cats.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fe1156c6-8bda-4506-9622-682d40361c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 USE SPACY TO PROCEED MORE ###\n",
    "#####################################\n",
    "\n",
    "# [2] tense verbs\n",
    "def label_tenses(file_path):\n",
    "    # 读取 CSV 文件，不指定列名\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # 存储结果\n",
    "    labeled_verbs = {\n",
    "        'past_tense': [],\n",
    "        'present_tense': [],\n",
    "        'future_tense': []\n",
    "    }\n",
    "\n",
    "    # # 遍历每一行文本\n",
    "    # for index in range(len(df)):\n",
    "    #     # 使用 spaCy 处理每一行文本\n",
    "    #     doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "    #     # 查找动词\n",
    "    #     for token in doc:\n",
    "       \n",
    "    #             if token.tag_ in ['VBD', 'VBN']:  # 过去时动词\n",
    "    #                 labeled_verbs['past_tense'].append(token.text)\n",
    "    #             elif token.tag_ in ['VBZ', 'VBP', 'VBG']:  # 现在时动词\n",
    "    #                 labeled_verbs['present_tense'].append(token.text)\n",
    "    #             elif token.tag_ == 'MD':  # 将来时动词（情态动词）\n",
    "                   \n",
    "    #                  if token.nbor().pos_ == \"VERB\":\n",
    "    #                     labeled_verbs['future_tense'].append(token.nbor().text)\n",
    "\n",
    "    # return labeled_verbs\n",
    "\n",
    "\n",
    "\n",
    "    # 遍历每一行文本\n",
    "    for index in range(len(df)):\n",
    "        # 使用 spaCy 处理每一行文本\n",
    "        doc = nlp(df.iloc[index, 0])  # 假设文本在第一列\n",
    "\n",
    "        # 查找动词\n",
    "        for token in doc:\n",
    "            # 检查动词的时态\n",
    "            if token.tag_ in ['VBD', 'VBN']:  # 过去时动词\n",
    "                labeled_verbs['past_tense'].append(token.text)\n",
    "            elif token.tag_ in ['VBZ', 'VBP', 'VBG']:  # 现在时动词\n",
    "                labeled_verbs['present_tense'].append(token.text)\n",
    "            elif token.tag_ == 'MD':  # 情态动词标记为将来时\n",
    "                # 确保 token 后面有单词，以防溢出\n",
    "                if token.i < len(doc) - 1 and doc[token.i + 1].pos_ == \"VERB\":\n",
    "                    # 将情态动词后的动词视为未来时态\n",
    "                    labeled_verbs['future_tense'].append(doc[token.i + 1].text)\n",
    "\n",
    "    return labeled_verbs\n",
    "\n",
    "# [3] numbers\n",
    "\n",
    "def label_numbers(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_numbers = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.like_num:  # 判断是否是数字\n",
    "                labeled_numbers.append(token.text)\n",
    "\n",
    "    return labeled_numbers\n",
    "\n",
    "\n",
    "# [4] prepositions\n",
    "\n",
    "def label_prepositions(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_prepositions = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"ADP\":  # 介词的 POS 标签是 ADP\n",
    "                labeled_prepositions.append(token.text)\n",
    "\n",
    "    return labeled_prepositions\n",
    "\n",
    "\n",
    "# [5] articles\n",
    "\n",
    "def label_articles(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_articles = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ == \"DET\":  # 冠词的 POS 标签是 DET\n",
    "                labeled_articles.append(token.text)\n",
    "\n",
    "    return labeled_articles\n",
    "\n",
    "\n",
    "# [6] negations\n",
    "\n",
    "def label_negations(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_negations = []\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "        for token in doc:\n",
    "            if token.dep_ == \"neg\":  # 否定词的依存关系标签是 neg\n",
    "                labeled_negations.append(token.text)\n",
    "\n",
    "    return labeled_negations\n",
    "\n",
    "# # [7] pronouns\n",
    "\n",
    "# def label_pronouns(file_path):\n",
    "#     df = pd.read_csv(file_path, header=None)\n",
    "#     labeled_pronouns = {\"first_person\": [], \"second_person\": [], \"third_person\": []}\n",
    "\n",
    "#     for index in range(len(df)):\n",
    "#         doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "\n",
    "#         for token in doc:\n",
    "#             if token.text in pronouns[\"first_person\"]:\n",
    "#                 labeled_pronouns[\"first_person\"].append(token.text)\n",
    "#             elif token.text in pronouns[\"second_person\"]:\n",
    "#                 labeled_pronouns[\"second_person\"].append(token.text)\n",
    "#             elif token.text in pronouns[\"third_person\"]:\n",
    "#                 labeled_pronouns[\"third_person\"].append(token.text)\n",
    "\n",
    "#     return labeled_pronouns\n",
    "\n",
    "# [8] empathy\n",
    "\n",
    "def label_empathy(file_path):\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    labeled_empathy = {label: [] for label in lexicon.cats.keys()}\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问每一行的文本\n",
    "        doc_text = doc.text.lower()  # 将文本转换为小写以匹配词汇表中的单词\n",
    "\n",
    "        for label, words in lexicon.cats.items():\n",
    "            for word in words:\n",
    "                if word in doc_text:\n",
    "                    labeled_empathy[label].append(word)\n",
    "\n",
    "    return labeled_empathy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea98e49-a355-4a86-a847-ce537ecd08ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 3. Data Processing\n",
    "\n",
    "Using the new lexicon, we start the data analysis of correlation between character personalty traits and their linguistic patterns. We measure frequency and percentage for each lexicon per character, comparing patterns based on personality dimensions.\n",
    "\n",
    "- *Percentage = Frequency / Number_of_Tokens\n",
    "\n",
    "The **personality scores** from our reference study (Stening and Stening 2018) are as follows:\n",
    "\n",
    "|   Character   | Neuroticism  | Extroversion |   Openness   |Agreeableness |Conscientiousness|\n",
    "|--------------|--------------|--------------|--------------|--------------|--------------|\n",
    "|Ron Wesley|3.22|4.9|4.02|3.76|3.01|\n",
    "|Hermine Granger|4.22|4.65|5.12|4.07|6.22|\n",
    "|Albus Dumbledore|5.52|4.36|5.52|5.07|5.73|\n",
    "|Lord Voldmort|3|4.36|4.27|1.95|4.88|\n",
    "|Draco Malfoy|3.15|4.23|3.86|2.15|4.16|\n",
    "|Harry Potter|3.85|3.92|5.13|4.11|4.36|\n",
    "|Severus Snape|4.43|2.65|4.08|2.6|5.49|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4e877e8-2706-464d-b3c3-f90aab3fc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analysis_tense(file_path):\n",
    "    # 调用不同的函数并打印结果\n",
    "    tenses_result = label_tenses(file_path)\n",
    "\n",
    "    # 计算总单词数\n",
    "    total_words = 0\n",
    "\n",
    "    # 计算每个句子的总单词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    for index in range(len(df)):\n",
    "        doc = nlp(df.iloc[index, 0])  # 访问第一列（每一行的文本）\n",
    "        total_words += len(doc)  # 统计当前句子的单词数\n",
    "\n",
    "    # 获取各类动词的数量\n",
    "    past_count = len(tenses_result['past_tense'])\n",
    "    present_count = len(tenses_result['present_tense'])\n",
    "    future_count = len(tenses_result['future_tense'])\n",
    "\n",
    "    # 计算百分比\n",
    "    past_percentage = (past_count / total_words) * 100 if total_words > 0 else 0\n",
    "    present_percentage = (present_count / total_words) * 100 if total_words > 0 else 0\n",
    "    future_percentage = (future_count / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Past Tense Verbs: {past_count} ({past_percentage:.2f}%)\")\n",
    "    print(f\"Present Tense Verbs: {present_count} ({present_percentage:.2f}%)\")\n",
    "    print(f\"Future Tense Verbs: {future_count} ({future_percentage:.2f}%)\")\n",
    "\n",
    "def analysis_numbers(file_path):\n",
    "    labeled_numbers = label_numbers(file_path)\n",
    "    \n",
    "    # 统计数字token的总数量\n",
    "    total_numbers = len(labeled_numbers)\n",
    "    \n",
    "    # 计算总词数（包含数字和非数字的总词数）\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算数字token的占比\n",
    "    number_percentage = (total_numbers / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Total number words: {total_numbers} ({number_percentage:.2f}%)\")\n",
    "    # print(f\"Total words containing numbers: {total_numbers}\")\n",
    "    # print(f\"Percentage of numbers in all tokens: {percentage:.2f}%\")\n",
    "\n",
    "\n",
    "# Analysis for prepositions\n",
    "def analysis_prepositions(file_path):\n",
    "    labeled_prepositions = label_prepositions(file_path)\n",
    "    \n",
    "    # 统计介词的总数量\n",
    "    total_prepositions = len(labeled_prepositions)\n",
    "    \n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算介词的占比\n",
    "    preposition_percentage = (total_prepositions / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Total prepositions: {total_prepositions} ({preposition_percentage:.2f}%)\")\n",
    "\n",
    "\n",
    "# Analysis for articles\n",
    "def analysis_articles(file_path):\n",
    "    labeled_articles = label_articles(file_path)\n",
    "    \n",
    "    # 统计冠词的总数量\n",
    "    total_articles = len(labeled_articles)\n",
    "    \n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算冠词的占比\n",
    "    article_percentage = (total_articles / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Total articles: {total_articles} ({article_percentage:.2f}%)\")\n",
    "\n",
    "\n",
    "# Analysis for negations\n",
    "def analysis_negations(file_path):\n",
    "    labeled_negations = label_negations(file_path)\n",
    "    \n",
    "    # 统计否定词的总数量\n",
    "    total_negations = len(labeled_negations)\n",
    "    \n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 计算否定词的占比\n",
    "    negation_percentage = (total_negations / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Total negations: {total_negations} ({negation_percentage:.2f}%)\")\n",
    "\n",
    "# def analysis_pronouns(file_path):\n",
    "#     labeled_pronouns = label_pronouns(file_path)\n",
    "\n",
    "#     # 统计每种代词的数量\n",
    "#     first_person_count = len(labeled_pronouns[\"first_person\"])\n",
    "#     second_person_count = len(labeled_pronouns[\"second_person\"])\n",
    "#     third_person_count = len(labeled_pronouns[\"third_person\"])\n",
    "\n",
    "#     # 计算总词数\n",
    "#     df = pd.read_csv(file_path, header=None)\n",
    "#     total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "#     # 计算各代词的百分比\n",
    "#     first_person_percentage = (first_person_count / total_words) * 100 if total_words > 0 else 0\n",
    "#     second_person_percentage = (second_person_count / total_words) * 100 if total_words > 0 else 0\n",
    "#     third_person_percentage = (third_person_count / total_words) * 100 if total_words > 0 else 0\n",
    "\n",
    "#     # 打印结果\n",
    "#     print(f\"First Person Pronouns: {first_person_count} ({first_person_percentage:.2f}%)\")\n",
    "#     print(f\"Second Person Pronouns: {second_person_count} ({second_person_percentage:.2f}%)\")\n",
    "#     print(f\"Third Person Pronouns: {third_person_count} ({third_person_percentage:.2f}%)\")\n",
    "\n",
    "def analysis_empathy(file_path):\n",
    "    labeled_empathy = label_empathy(file_path)\n",
    "\n",
    "    # 统计每个类别的词频\n",
    "    category_counts = {label: len(words) for label, words in labeled_empathy.items()}\n",
    "    total_empathy_words = sum(category_counts.values())\n",
    "\n",
    "    # 计算总词数\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    total_words = sum(len(nlp(row[0])) for row in df.values)\n",
    "    \n",
    "    # 打印每个类别的频率和百分比\n",
    "    print(f\"Total empathy-related words: {total_empathy_words} ({(total_empathy_words / total_words) * 100:.2f}% of total words)\")\n",
    "    for label, count in category_counts.items():\n",
    "        percentage = (count / total_words) * 100 if total_words > 0 else 0\n",
    "        print(f\"{label}, {count} ({percentage:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0fd2ce0a-158d-4742-95c2-769e75270e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement analysis and output results:\n",
    "\n",
    "def analysis_all(file_path):\n",
    "    \n",
    "    print(\"Tense Analysis:\")\n",
    "    analysis_tense(file_path)\n",
    "    \n",
    "    print(\"\\nNumber Analysis:\")\n",
    "    analysis_numbers(file_path)\n",
    "    \n",
    "    print(\"\\nPreposition Analysis:\")\n",
    "    analysis_prepositions(file_path)\n",
    "    \n",
    "    print(\"\\nArticle Analysis:\")\n",
    "    analysis_articles(file_path)\n",
    "    \n",
    "    print(\"\\nNegation Analysis:\")\n",
    "    analysis_negations(file_path)\n",
    "    \n",
    "    print(\"\\nEmpathy Analysis:\")\n",
    "    analysis_empathy(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1a8c83af-3012-4cca-ae55-76e4da1e447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts per character:\n",
      "\n",
      "Albus Dumbledore\n",
      "Tense Analysis:\n",
      "Past Tense Verbs: 386 (4.04%)\n",
      "Present Tense Verbs: 708 (7.42%)\n",
      "Future Tense Verbs: 109 (1.14%)\n",
      "\n",
      "Number Analysis:\n",
      "Total number words: 95 (1.00%)\n",
      "\n",
      "Preposition Analysis:\n",
      "Total prepositions: 619 (6.49%)\n",
      "\n",
      "Article Analysis:\n",
      "Total articles: 575 (6.03%)\n",
      "\n",
      "Negation Analysis:\n",
      "Total negations: 123 (1.29%)\n",
      "\n",
      "Empathy Analysis:\n",
      "Total empathy-related words: 5383 (56.41% of total words)\n",
      "money, 54 (0.57%)\n",
      "work, 1047 (10.97%)\n",
      "sleep, 48 (0.50%)\n",
      "occupation, 8 (0.08%)\n",
      "family, 92 (0.96%)\n",
      "swearing_terms, 31 (0.32%)\n",
      "leisure, 28 (0.29%)\n",
      "school, 76 (0.80%)\n",
      "optimism, 46 (0.48%)\n",
      "home, 62 (0.65%)\n",
      "sexuality, 10 (0.10%)\n",
      "religion, 48 (0.50%)\n",
      "body, 82 (0.86%)\n",
      "eating, 121 (1.27%)\n",
      "sports, 111 (1.16%)\n",
      "death, 100 (1.05%)\n",
      "communication, 140 (1.47%)\n",
      "hearing, 118 (1.24%)\n",
      "music, 149 (1.56%)\n",
      "sadness, 51 (0.53%)\n",
      "emotional, 47 (0.49%)\n",
      "affection, 19 (0.20%)\n",
      "anger, 30 (0.31%)\n",
      "negative_emotion, 151 (1.58%)\n",
      "friends, 113 (1.18%)\n",
      "achievement, 109 (1.14%)\n",
      "positive_emotion, 99 (1.04%)\n",
      "first_person_sg, 259 (2.71%)\n",
      "first_person_pl, 387 (4.06%)\n",
      "first_person, 646 (6.77%)\n",
      "second_person, 369 (3.87%)\n",
      "third_person, 732 (7.67%)\n",
      "\n",
      "Harry Potter\n",
      "Tense Analysis:\n",
      "Past Tense Verbs: 891 (4.57%)\n",
      "Present Tense Verbs: 1703 (8.74%)\n",
      "Future Tense Verbs: 159 (0.82%)\n",
      "\n",
      "Number Analysis:\n",
      "Total number words: 133 (0.68%)\n",
      "\n",
      "Preposition Analysis:\n",
      "Total prepositions: 900 (4.62%)\n",
      "\n",
      "Article Analysis:\n",
      "Total articles: 772 (3.96%)\n",
      "\n",
      "Negation Analysis:\n",
      "Total negations: 377 (1.93%)\n",
      "\n",
      "Empathy Analysis:\n",
      "Total empathy-related words: 10593 (54.33% of total words)\n",
      "money, 72 (0.37%)\n",
      "work, 3105 (15.93%)\n",
      "sleep, 80 (0.41%)\n",
      "occupation, 7 (0.04%)\n",
      "family, 123 (0.63%)\n",
      "swearing_terms, 58 (0.30%)\n",
      "leisure, 24 (0.12%)\n",
      "school, 44 (0.23%)\n",
      "optimism, 49 (0.25%)\n",
      "home, 147 (0.75%)\n",
      "sexuality, 9 (0.05%)\n",
      "religion, 79 (0.41%)\n",
      "body, 120 (0.62%)\n",
      "eating, 117 (0.60%)\n",
      "sports, 125 (0.64%)\n",
      "death, 150 (0.77%)\n",
      "communication, 234 (1.20%)\n",
      "hearing, 229 (1.17%)\n",
      "music, 222 (1.14%)\n",
      "sadness, 21 (0.11%)\n",
      "emotional, 78 (0.40%)\n",
      "affection, 13 (0.07%)\n",
      "anger, 10 (0.05%)\n",
      "negative_emotion, 364 (1.87%)\n",
      "friends, 158 (0.81%)\n",
      "achievement, 51 (0.26%)\n",
      "positive_emotion, 120 (0.62%)\n",
      "first_person_sg, 581 (2.98%)\n",
      "first_person_pl, 617 (3.16%)\n",
      "first_person, 1198 (6.14%)\n",
      "second_person, 640 (3.28%)\n",
      "third_person, 1748 (8.97%)\n",
      "\n",
      "Hermione Granger\n",
      "Tense Analysis:\n",
      "Past Tense Verbs: 456 (4.37%)\n",
      "Present Tense Verbs: 955 (9.16%)\n",
      "Future Tense Verbs: 84 (0.81%)\n",
      "\n",
      "Number Analysis:\n",
      "Total number words: 61 (0.59%)\n",
      "\n",
      "Preposition Analysis:\n",
      "Total prepositions: 519 (4.98%)\n",
      "\n",
      "Article Analysis:\n",
      "Total articles: 443 (4.25%)\n",
      "\n",
      "Negation Analysis:\n",
      "Total negations: 207 (1.99%)\n",
      "\n",
      "Empathy Analysis:\n",
      "Total empathy-related words: 5453 (52.31% of total words)\n",
      "money, 47 (0.45%)\n",
      "work, 1469 (14.09%)\n",
      "sleep, 44 (0.42%)\n",
      "occupation, 11 (0.11%)\n",
      "family, 67 (0.64%)\n",
      "swearing_terms, 35 (0.34%)\n",
      "leisure, 23 (0.22%)\n",
      "school, 49 (0.47%)\n",
      "optimism, 49 (0.47%)\n",
      "home, 58 (0.56%)\n",
      "sexuality, 14 (0.13%)\n",
      "religion, 21 (0.20%)\n",
      "body, 75 (0.72%)\n",
      "eating, 73 (0.70%)\n",
      "sports, 49 (0.47%)\n",
      "death, 52 (0.50%)\n",
      "communication, 116 (1.11%)\n",
      "hearing, 127 (1.22%)\n",
      "music, 121 (1.16%)\n",
      "sadness, 22 (0.21%)\n",
      "emotional, 50 (0.48%)\n",
      "affection, 15 (0.14%)\n",
      "anger, 23 (0.22%)\n",
      "negative_emotion, 196 (1.88%)\n",
      "friends, 102 (0.98%)\n",
      "achievement, 32 (0.31%)\n",
      "positive_emotion, 92 (0.88%)\n",
      "first_person_sg, 241 (2.31%)\n",
      "first_person_pl, 401 (3.85%)\n",
      "first_person, 642 (6.16%)\n",
      "second_person, 346 (3.32%)\n",
      "third_person, 791 (7.59%)\n",
      "\n",
      "Draco Malfoy\n",
      "Tense Analysis:\n",
      "Past Tense Verbs: 75 (4.32%)\n",
      "Present Tense Verbs: 155 (8.92%)\n",
      "Future Tense Verbs: 16 (0.92%)\n",
      "\n",
      "Number Analysis:\n",
      "Total number words: 12 (0.69%)\n",
      "\n",
      "Preposition Analysis:\n",
      "Total prepositions: 88 (5.06%)\n",
      "\n",
      "Article Analysis:\n",
      "Total articles: 78 (4.49%)\n",
      "\n",
      "Negation Analysis:\n",
      "Total negations: 39 (2.24%)\n",
      "\n",
      "Empathy Analysis:\n",
      "Total empathy-related words: 1078 (62.03% of total words)\n",
      "money, 3 (0.17%)\n",
      "work, 241 (13.87%)\n",
      "sleep, 4 (0.23%)\n",
      "occupation, 0 (0.00%)\n",
      "family, 25 (1.44%)\n",
      "swearing_terms, 9 (0.52%)\n",
      "leisure, 5 (0.29%)\n",
      "school, 11 (0.63%)\n",
      "optimism, 9 (0.52%)\n",
      "home, 22 (1.27%)\n",
      "sexuality, 0 (0.00%)\n",
      "religion, 6 (0.35%)\n",
      "body, 24 (1.38%)\n",
      "eating, 9 (0.52%)\n",
      "sports, 11 (0.63%)\n",
      "death, 13 (0.75%)\n",
      "communication, 20 (1.15%)\n",
      "hearing, 37 (2.13%)\n",
      "music, 27 (1.55%)\n",
      "sadness, 3 (0.17%)\n",
      "emotional, 7 (0.40%)\n",
      "affection, 2 (0.12%)\n",
      "anger, 2 (0.12%)\n",
      "negative_emotion, 36 (2.07%)\n",
      "friends, 27 (1.55%)\n",
      "achievement, 6 (0.35%)\n",
      "positive_emotion, 22 (1.27%)\n",
      "first_person_sg, 62 (3.57%)\n",
      "first_person_pl, 64 (3.68%)\n",
      "first_person, 126 (7.25%)\n",
      "second_person, 91 (5.24%)\n",
      "third_person, 154 (8.86%)\n",
      "\n",
      "Ron Wesley\n",
      "Tense Analysis:\n",
      "Past Tense Verbs: 385 (3.97%)\n",
      "Present Tense Verbs: 885 (9.14%)\n",
      "Future Tense Verbs: 50 (0.52%)\n",
      "\n",
      "Number Analysis:\n",
      "Total number words: 76 (0.78%)\n",
      "\n",
      "Preposition Analysis:\n",
      "Total prepositions: 468 (4.83%)\n",
      "\n",
      "Article Analysis:\n",
      "Total articles: 402 (4.15%)\n",
      "\n",
      "Negation Analysis:\n",
      "Total negations: 185 (1.91%)\n",
      "\n",
      "Empathy Analysis:\n",
      "Total empathy-related words: 5175 (53.43% of total words)\n",
      "money, 28 (0.29%)\n",
      "work, 1498 (15.47%)\n",
      "sleep, 47 (0.49%)\n",
      "occupation, 7 (0.07%)\n",
      "family, 72 (0.74%)\n",
      "swearing_terms, 62 (0.64%)\n",
      "leisure, 17 (0.18%)\n",
      "school, 17 (0.18%)\n",
      "optimism, 31 (0.32%)\n",
      "home, 76 (0.78%)\n",
      "sexuality, 6 (0.06%)\n",
      "religion, 10 (0.10%)\n",
      "body, 66 (0.68%)\n",
      "eating, 74 (0.76%)\n",
      "sports, 45 (0.46%)\n",
      "death, 50 (0.52%)\n",
      "communication, 80 (0.83%)\n",
      "hearing, 101 (1.04%)\n",
      "music, 106 (1.09%)\n",
      "sadness, 17 (0.18%)\n",
      "emotional, 44 (0.45%)\n",
      "affection, 8 (0.08%)\n",
      "anger, 10 (0.10%)\n",
      "negative_emotion, 185 (1.91%)\n",
      "friends, 98 (1.01%)\n",
      "achievement, 30 (0.31%)\n",
      "positive_emotion, 61 (0.63%)\n",
      "first_person_sg, 249 (2.57%)\n",
      "first_person_pl, 328 (3.39%)\n",
      "first_person, 577 (5.96%)\n",
      "second_person, 317 (3.27%)\n",
      "third_person, 858 (8.86%)\n",
      "\n",
      "Severus Snape\n",
      "Tense Analysis:\n",
      "Past Tense Verbs: 105 (3.49%)\n",
      "Present Tense Verbs: 221 (7.35%)\n",
      "Future Tense Verbs: 34 (1.13%)\n",
      "\n",
      "Number Analysis:\n",
      "Total number words: 22 (0.73%)\n",
      "\n",
      "Preposition Analysis:\n",
      "Total prepositions: 219 (7.29%)\n",
      "\n",
      "Article Analysis:\n",
      "Total articles: 175 (5.82%)\n",
      "\n",
      "Negation Analysis:\n",
      "Total negations: 38 (1.26%)\n",
      "\n",
      "Empathy Analysis:\n",
      "Total empathy-related words: 1660 (55.24% of total words)\n",
      "money, 12 (0.40%)\n",
      "work, 336 (11.18%)\n",
      "sleep, 16 (0.53%)\n",
      "occupation, 4 (0.13%)\n",
      "family, 22 (0.73%)\n",
      "swearing_terms, 11 (0.37%)\n",
      "leisure, 8 (0.27%)\n",
      "school, 21 (0.70%)\n",
      "optimism, 16 (0.53%)\n",
      "home, 23 (0.77%)\n",
      "sexuality, 2 (0.07%)\n",
      "religion, 12 (0.40%)\n",
      "body, 31 (1.03%)\n",
      "eating, 20 (0.67%)\n",
      "sports, 16 (0.53%)\n",
      "death, 25 (0.83%)\n",
      "communication, 41 (1.36%)\n",
      "hearing, 25 (0.83%)\n",
      "music, 33 (1.10%)\n",
      "sadness, 18 (0.60%)\n",
      "emotional, 8 (0.27%)\n",
      "affection, 3 (0.10%)\n",
      "anger, 7 (0.23%)\n",
      "negative_emotion, 37 (1.23%)\n",
      "friends, 42 (1.40%)\n",
      "achievement, 15 (0.50%)\n",
      "positive_emotion, 27 (0.90%)\n",
      "first_person_sg, 88 (2.93%)\n",
      "first_person_pl, 137 (4.56%)\n",
      "first_person, 225 (7.49%)\n",
      "second_person, 165 (5.49%)\n",
      "third_person, 214 (7.12%)\n",
      "\n",
      "Lord Voldemort\n",
      "Tense Analysis:\n",
      "Past Tense Verbs: 78 (3.60%)\n",
      "Present Tense Verbs: 152 (7.02%)\n",
      "Future Tense Verbs: 31 (1.43%)\n",
      "\n",
      "Number Analysis:\n",
      "Total number words: 14 (0.65%)\n",
      "\n",
      "Preposition Analysis:\n",
      "Total prepositions: 112 (5.17%)\n",
      "\n",
      "Article Analysis:\n",
      "Total articles: 84 (3.88%)\n",
      "\n",
      "Negation Analysis:\n",
      "Total negations: 36 (1.66%)\n",
      "\n",
      "Empathy Analysis:\n",
      "Total empathy-related words: 1370 (63.25% of total words)\n",
      "money, 11 (0.51%)\n",
      "work, 272 (12.56%)\n",
      "sleep, 6 (0.28%)\n",
      "occupation, 2 (0.09%)\n",
      "family, 24 (1.11%)\n",
      "swearing_terms, 3 (0.14%)\n",
      "leisure, 3 (0.14%)\n",
      "school, 4 (0.18%)\n",
      "optimism, 12 (0.55%)\n",
      "home, 26 (1.20%)\n",
      "sexuality, 4 (0.18%)\n",
      "religion, 5 (0.23%)\n",
      "body, 25 (1.15%)\n",
      "eating, 25 (1.15%)\n",
      "sports, 10 (0.46%)\n",
      "death, 38 (1.75%)\n",
      "communication, 20 (0.92%)\n",
      "hearing, 22 (1.02%)\n",
      "music, 25 (1.15%)\n",
      "sadness, 13 (0.60%)\n",
      "emotional, 8 (0.37%)\n",
      "affection, 4 (0.18%)\n",
      "anger, 7 (0.32%)\n",
      "negative_emotion, 52 (2.40%)\n",
      "friends, 31 (1.43%)\n",
      "achievement, 12 (0.55%)\n",
      "positive_emotion, 24 (1.11%)\n",
      "first_person_sg, 84 (3.88%)\n",
      "first_person_pl, 111 (5.12%)\n",
      "first_person, 195 (9.00%)\n",
      "second_person, 119 (5.49%)\n",
      "third_person, 173 (7.99%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Counts per character:\")\n",
    "\n",
    "print(\"\\nAlbus Dumbledore\")\n",
    "analysis_all(\"Tokens/Dumbledore.csv\")\n",
    "\n",
    "print(\"\\nHarry Potter\")\n",
    "analysis_all(\"Tokens/Harry.csv\")\n",
    "\n",
    "print(\"\\nHermione Granger\")\n",
    "analysis_all(\"Tokens/Hermione.csv\")\n",
    "\n",
    "print(\"\\nDraco Malfoy\")\n",
    "analysis_all(\"Tokens/Malfoy.csv\")\n",
    "\n",
    "print(\"\\nRon Wesley\")\n",
    "analysis_all(\"Tokens/Ron.csv\")\n",
    "\n",
    "print(\"\\nSeverus Snape\")\n",
    "analysis_all(\"Tokens/Snape.csv\")\n",
    "\n",
    "print(\"\\nLord Voldemort\")\n",
    "analysis_all(\"Tokens/Voldemort.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3764acf8-8f31-493f-bf52-5deea98b8d1f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Results on Language Use per Character\n",
    "\n",
    "| Label                          | Albus Dumbledore   | Harry Potter       | Hermione Granger    | Draco Malfoy       | Ron Wesley       | Severus Snape     | Lord Voldemort    |\n",
    "|--------------------------------|--------------------|--------------------|---------------------|--------------------|------------------|-------------------|-------------------|\n",
    "| **Tense Analysis**             |                    |                    |                     |                    |                  |                   |                   |\n",
    "| Past Tense Verbs               | 386 (4.04%)       | 891 (4.57%)       | 456 (4.37%)        | 75 (4.32%)        | 385 (3.97%)     | 105 (3.49%)      | 78 (3.60%)       |\n",
    "| Present Tense Verbs            | 708 (7.42%)       | 1703 (8.74%)      | 955 (9.16%)        | 155 (8.92%)       | 885 (9.14%)     | 221 (7.35%)      | 152 (7.02%)      |\n",
    "| Future Tense Verbs             | 109 (1.14%)       | 159 (0.82%)       | 84 (0.81%)         | 16 (0.92%)        | 50 (0.52%)      | 34 (1.13%)       | 31 (1.43%)       |\n",
    "| **Number Analysis**            |                    |                    |                     |                    |                  |                   |                   |\n",
    "| Total Number Words             | 95 (1.00%)        | 133 (0.68%)       | 61 (0.59%)         | 12 (0.69%)        | 76 (0.78%)      | 22 (0.73%)       | 14 (0.65%)       |\n",
    "| **Preposition Analysis**       |                    |                    |                     |                    |                  |                   |                   |\n",
    "| Total Prepositions             | 619 (6.49%)       | 900 (4.62%)       | 519 (4.98%)        | 88 (5.06%)        | 468 (4.83%)     | 219 (7.29%)      | 112 (5.17%)      |\n",
    "| **Article Analysis**           |                    |                    |                     |                    |                  |                   |                   |\n",
    "| Total Articles                 | 575 (6.03%)       | 772 (3.96%)       | 443 (4.25%)        | 78 (4.49%)        | 402 (4.15%)     | 175 (5.82%)      | 84 (3.88%)       |\n",
    "| **Negation Analysis**          |                    |                    |                     |                    |                  |                   |                   |\n",
    "| Total Negations                | 123 (1.29%)       | 377 (1.93%)       | 207 (1.99%)        | 39 (2.24%)        | 185 (1.91%)     | 38 (1.26%)       | 36 (1.66%)       |\n",
    "| **Pronoun Analysis**           |                    |                    |                     |                    |                  |                   |                   |\n",
    "| First Person Singular          | 259 (2.71%)      | 581 (2.98%)       | 241 (2.31%)        | 62 (3.57%)        | 249 (2.57%)      | 88 (2.93%)       | 84 (3.88%)       |\n",
    "| First Person Plural            | 387 (4.06%)      | 617 (3.16%)       | 401 (3.85%)        | 64 (3.68%)        | 328 (3.39%)      | 137 (4.56%)      | 111 (5.12%)      |\n",
    "| First Person                   | 646 (6.77%)      | 1198 (6.14%)      | 642 (6.16%)        | 126 (7.25%)       | 577 (5.96%)      | 225 (7.49%)      | 195 (9.00%)      |\n",
    "| Second Person                  | 369 (3.87%)      | 640 (3.28%)       | 346 (3.32%)        | 91 (5.24%)        | 317 (3.27%)      | 165 (5.49%)      | 119 (5.49%)      |\n",
    "| Third Person                   | 732 (7.67%)      | 1748 (8.97%)      | 791 (7.59%)        | 154 (8.86%)       | 858 (8.86%)      | 214 (7.12%)      | 173 (7.99%)      |\n",
    "| **Empathy Analysis**           |                  |                   |                    |                   |                  |                  |                  |\n",
    "| Total Empathy-related Words    | 5391 (56.49%)    | 10603 (54.39%)    | 5464 (52.42%)      | 1079 (62.08%)     | 5179 (53.47%)    | 1662 (55.31%)    | 1371 (63.30%)    |\n",
    "| Money                          | 54 (0.57%)       | 72 (0.37%)        | 47 (0.45%)         | 3 (0.17%)         | 28 (0.29%)       | 12 (0.40%)       | 11 (0.51%)       |\n",
    "| Work                           | 1047 (10.97%)    | 3105 (15.93%)     | 1469 (14.09%)      | 241 (13.87%)      | 1498 (15.47%)    | 336 (11.18%)     | 272 (12.56%)     |\n",
    "| Sleep                          | 48 (0.50%)       | 80 (0.41%)        | 44 (0.42%)         | 4 (0.23%)         | 47 (0.49%)       | 16 (0.53%)       | 6 (0.28%)        |\n",
    "| Occupation                     | 8 (0.08%)        | 7 (0.04%)         | 11 (0.11%)         | 0 (0.00%)         | 7 (0.07%)        | 4 (0.13%)        | 2 (0.09%)        |\n",
    "| Family                         | 92 (0.96%)       | 123 (0.63%)       | 67 (0.64%)         | 25 (1.44%)        | 72 (0.74%)       | 22 (0.73%)       | 24 (1.11%)       |\n",
    "| Swearing Terms                 | 31 (0.32%)       | 58 (0.30%)        | 35 (0.34%)         | 9 (0.52%)         | 62 (0.64%)       | 11 (0.37%)       | 3 (0.14%)        |\n",
    "| Leisure                        | 28 (0.29%)       | 24 (0.12%)        | 23 (0.22%)         | 5 (0.29%)         | 17 (0.18%)       | 8 (0.27%)        | 3 (0.14%)        |\n",
    "| School                         | 76 (0.80%)       | 44 (0.23%)        | 49 (0.47%)         | 11 (0.63%)        | 17 (0.18%)       | 21 (0.70%)       | 4 (0.18%)        |\n",
    "| Optimism                       | 46 (0.48%)       | 49 (0.25%)        | 49 (0.47%)         | 9 (0.52%)         | 31 (0.32%)       | 16 (0.53%)       | 12 (0.55%)       |\n",
    "| Home                           | 62 (0.65%)       | 147 (0.75%)       | 58 (0.56%)         | 22 (1.27%)        | 76 (0.78%)       | 23 (0.77%)       | 26 (1.20%)       |\n",
    "| Sexuality                      | 10 (0.10%)       | 9 (0.05%)         | 14 (0.13%)         | 0 (0.00%)         | 6 (0.06%)        | 2 (0.07%)        | 4 (0.18%)        |\n",
    "| Superhero                      | 8 (0.08%)        | 10 (0.05%)        | 11 (0.11%)         | 1 (0.06%)         | 4 (0.04%)        | 2 (0.07%)        | 1 (0.05%)        |\n",
    "| Religion                       | 48 (0.50%)       | 79 (0.41%)        | 21 (0.20%)         | 6 (0.35%)         | 10 (0.10%)       | 12 (0.40%)       | 5 (0.23%)        |\n",
    "| Body                           | 82 (0.86%)       | 120 (0.62%)       | 75 (0.72%)         | 24 (1.38%)        | 66 (0.68%)       | 31 (1.03%)       | 25 (1.15%)       |\n",
    "| Eating                         | 121 (1.27%)      | 117 (0.60%)       | 73 (0.70%)         | 9 (0.52%)         | 74 (0.76%)       | 20 (0.67%)       | 25 (1.15%)       |\n",
    "| Sports                         | 111 (1.16%)      | 125 (0.64%)       | 49 (0.47%)         | 11 (0.63%)        | 45 (0.46%)       | 16 (0.53%)       | 10 (0.46%)       |\n",
    "| Death                          | 100 (1.05%)      | 150 (0.77%)       | 52 (0.50%)         | 13 (0.75%)        | 50 (0.52%)       | 25 (0.83%)       | 38 (1.75%)       |\n",
    "| Communication                  | 140 (1.47%)      | 234 (1.20%)       | 116 (1.11%)        | 20 (1.15%)        | 80 (0.83%)       | 41 (1.36%)       | 20 (0.92%)       |\n",
    "| Hearing                        | 118 (1.24%)      | 229 (1.17%)       | 127 (1.22%)        | 37 (2.13%)        | 101 (1.04%)      | 25 (0.83%)       | 22 (1.02%)       |\n",
    "| Music                          | 149 (1.56%)      | 222 (1.14%)       | 121 (1.16%)        | 27 (1.55%)        | 106 (1.09%)      | 33 (1.10%)       | 25 (1.15%)       |\n",
    "| Sadness                        | 51 (0.53%)       | 21 (0.11%)        | 22 (0.21%)         | 3 (0.17%)         | 17 (0.18%)       | 18 (0.60%)       | 13 (0.60%)       |\n",
    "| Emotional                      | 47 (0.49%)       | 78 (0.40%)        | 50 (0.48%)         | 7 (0.40%)         | 44 (0.45%)       | 8 (0.27%)        | 8 (0.37%)        |\n",
    "| Affection                      | 19 (0.20%)       | 13 (0.07%)        | 15 (0.14%)         | 2 (0.12%)         | 8 (0.08%)        | 3 (0.10%)        | 4 (0.18%)        |\n",
    "| Anger                          | 30 (0.31%)       | 10 (0.05%)        | 23 (0.22%)         | 2 (0.12%)         | 10 (0.10%)       | 7 (0.23%)        | 7 (0.32%)        |\n",
    "| Negative Emotion               | 151 (1.58%)      | 364 (1.87%)       | 196 (1.88%)        | 36 (2.07%)        | 185 (1.91%)      | 37 (1.23%)       | 52 (2.40%)       |\n",
    "| Friends                        | 113 (1.18%)      | 158 (0.81%)       | 102 (0.98%)        | 27 (1.55%)        | 98 (1.01%)       | 42 (1.40%)       | 31 (1.43%)       |\n",
    "| Achievement                    | 109 (1.14%)      | 51 (0.26%)        | 32 (0.31%)         | 6 (0.35%)         | 30 (0.31%)       | 15 (0.50%)       | 12 (0.55%)       |\n",
    "| Positive Emotion               | 99 (1.04%)       | 120 (0.62%)       | 92 (0.88%)         | 22 (1.27%)        | 61 (0.63%)       | 27 (0.90%)       | 24 (1.11%)       |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5d116b-6689-4d61-9ea1-6d481496e560",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 4. Data Analysis\n",
    "\n",
    "Now we conduct the statistical tests to examine our hypotheses. <br>\n",
    "\n",
    "**Normal Distribution** <br>\n",
    "Prior to determining the appropriate correlation test (Spearman or Pearson), we assess the normality of our data distribution. <br>\n",
    "\n",
    "**Correlation Test**\n",
    "Results indicate that `total prepositions`, `articles`, `sports`, `hearing`, `music`, `achievement`, `sadness` are deviated from a normal distribution. So we will apply _Spearman_ test for these variables and the _Pearson_ test for the remaining categories. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dc294eca-6127-41f7-8cd0-101d76de28d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages for statistical analysis\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import pearsonr, spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "576f8570-4ee3-44ed-8c5b-ece4df03e489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Past Tense Verbs: p-value = 0.66796\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Present Tense Verbs: p-value = 0.06280\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Future Tense Verbs: p-value = 0.92010\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Total Number Words: p-value = 0.16491\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Total Prepositions: p-value = 0.04550\n",
      "  The data is NOT normally distributed (p <= 0.05)\n",
      "\n",
      "Total Articles: p-value = 0.03818\n",
      "  The data is NOT normally distributed (p <= 0.05)\n",
      "\n",
      "Total Negations: p-value = 0.38827\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "First Person Singular: p-value = 0.65063\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "First Person Plural: p-value = 0.80727\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "First Person: p-value = 0.17471\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Second Person: p-value = 0.08490\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Third Person: p-value = 0.97860\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Total Empathy-related Words: p-value = 0.16531\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Money: p-value = 0.96260\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Work: p-value = 0.51883\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Sleep: p-value = 0.30898\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Occupation: p-value = 0.90013\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Family: p-value = 0.16767\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Swearing Terms: p-value = 0.72290\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Leisure: p-value = 0.28709\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "School: p-value = 0.20633\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Optimism: p-value = 0.09153\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Home: p-value = 0.11494\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Sexuality: p-value = 0.96346\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Superhero: p-value = 0.42894\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Religion: p-value = 0.80935\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Body: p-value = 0.55436\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Eating: p-value = 0.13419\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Sports: p-value = 0.00417\n",
      "  The data is NOT normally distributed (p <= 0.05)\n",
      "\n",
      "Death: p-value = 0.07719\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Communication: p-value = 0.90123\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Hearing: p-value = 0.02155\n",
      "  The data is NOT normally distributed (p <= 0.05)\n",
      "\n",
      "Music: p-value = 0.00431\n",
      "  The data is NOT normally distributed (p <= 0.05)\n",
      "\n",
      "Sadness: p-value = 0.03735\n",
      "  The data is NOT normally distributed (p <= 0.05)\n",
      "\n",
      "Emotional: p-value = 0.45892\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Affection: p-value = 0.64322\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Anger: p-value = 0.48955\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Negative Emotion: p-value = 0.81247\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Friends: p-value = 0.65294\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Achievement: p-value = 0.01122\n",
      "  The data is NOT normally distributed (p <= 0.05)\n",
      "\n",
      "Positive Emotion: p-value = 0.64506\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DISTRIBUTION TEST ON LEXICON DATA ###\n",
    "\n",
    "# Defining the data for each category based on the percentages provided for each character\n",
    "data = {\n",
    "    \"Past Tense Verbs\": [4.04, 4.57, 4.37, 4.32, 3.97, 3.49, 3.60],\n",
    "    \"Present Tense Verbs\": [7.42, 8.74, 9.16, 8.92, 9.14, 7.35, 7.02],\n",
    "    \"Future Tense Verbs\": [1.14, 0.82, 0.81, 0.92, 0.52, 1.13, 1.43],\n",
    "    \"Total Number Words\": [1.00, 0.68, 0.59, 0.69, 0.78, 0.73, 0.65],\n",
    "    \"Total Prepositions\": [6.49, 4.62, 4.98, 5.06, 4.83, 7.29, 5.17],\n",
    "    \"Total Articles\": [6.03, 3.96, 4.25, 4.49, 4.15, 5.82, 3.88],\n",
    "    \"Total Negations\": [1.29, 1.93, 1.99, 2.24, 1.91, 1.26, 1.66],\n",
    "    \"First Person Singular\": [2.71, 2.98, 2.31, 3.57, 2.57, 2.93, 3.88],\n",
    "    \"First Person Plural\": [4.06, 3.16, 3.85, 3.68, 3.39, 4.56, 5.12],\n",
    "    \"First Person\": [6.78, 6.14, 5.78, 7.25, 5.96, 6.89, 9.00],\n",
    "    \"Second Person\": [3.11, 2.49, 2.57, 3.74, 2.47, 3.96, 3.83],\n",
    "    \"Third Person\": [1.54, 1.84, 1.62, 1.21, 1.97, 1.36, 1.71],\n",
    "    \"Total Empathy-related Words\": [56.49, 54.39, 52.42, 62.08, 53.47, 55.31, 63.30],\n",
    "    \"Money\": [0.57, 0.37, 0.45, 0.17, 0.29, 0.40, 0.51],\n",
    "    \"Work\": [10.97, 15.93, 14.09, 13.87, 15.47, 11.18, 12.56],\n",
    "    \"Sleep\": [0.50, 0.41, 0.42, 0.23, 0.49, 0.53, 0.28],\n",
    "    \"Occupation\": [0.08, 0.04, 0.11, 0.00, 0.07, 0.13, 0.09],\n",
    "    \"Family\": [0.96, 0.63, 0.64, 1.44, 0.74, 0.73, 1.11],\n",
    "    \"Swearing Terms\": [0.32, 0.30, 0.34, 0.52, 0.64, 0.37, 0.14],\n",
    "    \"Leisure\": [0.29, 0.12, 0.22, 0.29, 0.18, 0.27, 0.14],\n",
    "    \"School\": [0.80, 0.23, 0.47, 0.63, 0.18, 0.70, 0.18],\n",
    "    \"Optimism\": [0.48, 0.25, 0.47, 0.52, 0.32, 0.53, 0.55],\n",
    "    \"Home\": [0.65, 0.75, 0.56, 1.27, 0.78, 0.77, 1.20],\n",
    "    \"Sexuality\": [0.10, 0.05, 0.13, 0.00, 0.06, 0.07, 0.18],\n",
    "    \"Superhero\": [0.08, 0.05, 0.11, 0.06, 0.04, 0.07, 0.05],\n",
    "    \"Religion\": [0.50, 0.41, 0.20, 0.35, 0.10, 0.40, 0.23],\n",
    "    \"Body\": [0.86, 0.62, 0.72, 1.38, 0.68, 1.03, 1.15],\n",
    "    \"Eating\": [1.27, 0.60, 0.70, 0.52, 0.76, 0.67, 1.15],\n",
    "    \"Sports\": [1.16, 0.64, 0.47, 0.63, 0.46, 0.53, 0.46],\n",
    "    \"Death\": [1.05, 0.77, 0.50, 0.75, 0.52, 0.83, 1.75],\n",
    "    \"Communication\": [1.47, 1.20, 1.11, 1.15, 0.83, 1.36, 0.92],\n",
    "    \"Hearing\": [1.24, 1.17, 1.22, 2.13, 1.04, 0.83, 1.02],\n",
    "    \"Music\": [1.56, 1.14, 1.16, 1.55, 1.09, 1.10, 1.15],\n",
    "    \"Sadness\": [0.53, 0.11, 0.21, 0.17, 0.18, 0.60, 0.60],\n",
    "    \"Emotional\": [0.49, 0.40, 0.48, 0.40, 0.45, 0.27, 0.37],\n",
    "    \"Affection\": [0.20, 0.07, 0.14, 0.12, 0.08, 0.10, 0.18],\n",
    "    \"Anger\": [0.31, 0.05, 0.22, 0.12, 0.10, 0.23, 0.32],\n",
    "    \"Negative Emotion\": [1.58, 1.87, 1.88, 2.07, 1.91, 1.23, 2.40],\n",
    "    \"Friends\": [1.18, 0.81, 0.98, 1.55, 1.01, 1.40, 1.43],\n",
    "    \"Achievement\": [1.14, 0.26, 0.31, 0.35, 0.31, 0.50, 0.55],\n",
    "    \"Positive Emotion\": [1.04, 0.62, 0.88, 1.27, 0.63, 0.90, 1.11],\n",
    "}\n",
    "\n",
    "\n",
    "# Perform Shapiro-Wilk test for each dataset\n",
    "for label, values in data.items():\n",
    "    stat, p_value = shapiro(values)\n",
    "    print(f\"{label}: p-value = {p_value:.5f}\")\n",
    "    if p_value > 0.05:\n",
    "        print(\"  The data is normally distributed (p > 0.05)\\n\")\n",
    "    else:\n",
    "        print(\"  The data is NOT normally distributed (p <= 0.05)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "56bfaad8-1da1-483f-8181-d2a7f41ff5c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuroticism: p-value = 0.394\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Extroversion: p-value = 0.098\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Openness: p-value = 0.176\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Agreeableness: p-value = 0.485\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n",
      "Conscientiousness: p-value = 0.896\n",
      "  The data is normally distributed (p > 0.05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### DISTRIBUTION TEST ON PERSONALITY DATA ###\n",
    "\n",
    "# Big Five 各个性格特征的评分数据\n",
    "neuroticism = [3.22, 4.22, 5.52, 3.0, 3.15, 3.85, 4.43]\n",
    "extroversion = [4.9, 4.65, 4.36, 4.36, 4.23, 3.92, 2.65]\n",
    "openness = [4.02, 5.12, 5.52, 4.27, 3.86, 5.13, 4.08]\n",
    "agreeableness = [3.76, 4.07, 5.07, 1.95, 2.15, 4.11, 2.6]\n",
    "conscientiousness = [3.01, 6.22, 5.73, 4.88, 4.16, 4.36, 5.49]\n",
    "\n",
    "# 将各维度的结果存储在字典中，方便批量检验\n",
    "big_five_data = {\n",
    "    \"Neuroticism\": neuroticism,\n",
    "    \"Extroversion\": extroversion,\n",
    "    \"Openness\": openness,\n",
    "    \"Agreeableness\": agreeableness,\n",
    "    \"Conscientiousness\": conscientiousness\n",
    "}\n",
    "\n",
    "# 对每个性格特征进行 Shapiro-Wilk 检验并打印 p-value 和判断结果\n",
    "for trait, scores in big_five_data.items():\n",
    "    stat, p_value = shapiro(scores)\n",
    "    print(f\"{trait}: p-value = {p_value:.3f}\")\n",
    "    if p_value > 0.05:\n",
    "        print(f\"  The data is normally distributed (p > 0.05)\\n\")\n",
    "    else:\n",
    "        print(f\"  The data is NOT normally distributed (p <= 0.05)\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d9b2a958-7cf8-4f7a-b587-1eda7f60e358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neuroticism Correlations:\n",
      "  Past Tense Verbs: Pearson correlation = 0.138, p-value = 0.76863\n",
      "  Present Tense Verbs: Pearson correlation = 0.063, p-value = 0.89387\n",
      "  Future Tense Verbs: Pearson correlation = 0.123, p-value = 0.79203\n",
      "  Total Number Words: Pearson correlation = -0.658, p-value = 0.10805\n",
      "  Total Prepositions: Pearson correlation = -0.216, p-value = 0.64186\n",
      "  Total Articles: Pearson correlation = -0.365, p-value = 0.42100\n",
      "  Total Negations: Pearson correlation = 0.088, p-value = 0.85142\n",
      "  First Person Singular: Pearson correlation = -0.216, p-value = 0.64155\n",
      "  First Person Plural: Pearson correlation = 0.208, p-value = 0.65493\n",
      "  First Person: Pearson correlation = -0.079, p-value = 0.86669\n",
      "  Second Person: Pearson correlation = -0.228, p-value = 0.62222\n",
      "  Third Person: Pearson correlation = 0.206, p-value = 0.65838\n",
      "  Total Empathy-related Words: Pearson correlation = -0.279, p-value = 0.54507\n",
      "  Money: Pearson correlation = 0.418, p-value = 0.35046\n",
      "  Work: Pearson correlation = 0.120, p-value = 0.79736\n",
      "  Sleep: Pearson correlation = -0.029, p-value = 0.95103\n",
      "  Occupation: Pearson correlation = 0.505, p-value = 0.24725\n",
      "  Family: Pearson correlation = -0.494, p-value = 0.26018\n",
      "  Swearing Terms: Pearson correlation = -0.571, p-value = 0.18064\n",
      "  Leisure: Pearson correlation = -0.411, p-value = 0.35970\n",
      "  School: Pearson correlation = -0.277, p-value = 0.54715\n",
      "  Optimism: Pearson correlation = 0.051, p-value = 0.91282\n",
      "  Home: Pearson correlation = -0.342, p-value = 0.45224\n",
      "  Sexuality: Pearson correlation = 0.627, p-value = 0.13183\n",
      "  Superhero: Pearson correlation = 0.570, p-value = 0.18122\n",
      "  Religion: Pearson correlation = -0.266, p-value = 0.56491\n",
      "  Body: Pearson correlation = -0.330, p-value = 0.46973\n",
      "  Eating: Pearson correlation = -0.047, p-value = 0.92017\n",
      "  Sports: Pearson correlation = -0.426, p-value = 0.34066\n",
      "  Death: Pearson correlation = 0.037, p-value = 0.93796\n",
      "  Communication: Pearson correlation = -0.130, p-value = 0.78058\n",
      "  Hearing: Pearson correlation = -0.355, p-value = 0.43491\n",
      "  Music: Pearson correlation = -0.519, p-value = 0.23267\n",
      "  Sadness: Pearson correlation = -0.013, p-value = 0.97750\n",
      "  Emotional: Pearson correlation = 0.051, p-value = 0.91376\n",
      "  Affection: Pearson correlation = 0.091, p-value = 0.84612\n",
      "  Anger: Pearson correlation = 0.211, p-value = 0.65047\n",
      "  Negative Emotion: Pearson correlation = 0.160, p-value = 0.73127\n",
      "  Friends: Pearson correlation = -0.357, p-value = 0.43243\n",
      "  Achievement: Pearson correlation = -0.312, p-value = 0.49634\n",
      "  Positive Emotion: Pearson correlation = -0.183, p-value = 0.69406\n",
      "\n",
      "Extroversion Correlations:\n",
      "  Past Tense Verbs: Pearson correlation = 0.666, p-value = 0.10256\n",
      "  Present Tense Verbs: Pearson correlation = 0.503, p-value = 0.25022\n",
      "  Future Tense Verbs: Pearson correlation = -0.584, p-value = 0.16827\n",
      "  Total Number Words: Pearson correlation = 0.453, p-value = 0.30723\n",
      "  Total Prepositions: Pearson correlation = 0.030, p-value = 0.94935\n",
      "  Total Articles: Pearson correlation = 0.362, p-value = 0.42471\n",
      "  Total Negations: Pearson correlation = 0.098, p-value = 0.83425\n",
      "  First Person Singular: Pearson correlation = -0.653, p-value = 0.11183\n",
      "  First Person Plural: Pearson correlation = -0.782, p-value = 0.03787\n",
      "  First Person: Pearson correlation = -0.800, p-value = 0.03075\n",
      "  Second Person: Pearson correlation = -0.550, p-value = 0.20098\n",
      "  Third Person: Pearson correlation = -0.072, p-value = 0.87889\n",
      "  Total Empathy-related Words: Pearson correlation = -0.585, p-value = 0.16753\n",
      "  Money: Pearson correlation = -0.167, p-value = 0.72008\n",
      "  Work: Pearson correlation = 0.189, p-value = 0.68471\n",
      "  Sleep: Pearson correlation = 0.410, p-value = 0.36070\n",
      "  Occupation: Pearson correlation = -0.302, p-value = 0.51046\n",
      "  Family: Pearson correlation = -0.244, p-value = 0.59761\n",
      "  Swearing Terms: Pearson correlation = 0.451, p-value = 0.31036\n",
      "  Leisure: Pearson correlation = 0.394, p-value = 0.38193\n",
      "  School: Pearson correlation = 0.453, p-value = 0.30792\n",
      "  Optimism: Pearson correlation = -0.470, p-value = 0.28705\n",
      "  Home: Pearson correlation = -0.574, p-value = 0.17811\n",
      "  Sexuality: Pearson correlation = -0.606, p-value = 0.14933\n",
      "  Superhero: Pearson correlation = 0.303, p-value = 0.50939\n",
      "  Religion: Pearson correlation = 0.422, p-value = 0.34534\n",
      "  Body: Pearson correlation = -0.432, p-value = 0.33255\n",
      "  Eating: Pearson correlation = -0.260, p-value = 0.57355\n",
      "  Sports: Pearson correlation = 0.578, p-value = 0.17387\n",
      "  Death: Pearson correlation = -0.725, p-value = 0.06525\n",
      "  Communication: Pearson correlation = 0.533, p-value = 0.21844\n",
      "  Hearing: Pearson correlation = 0.302, p-value = 0.50984\n",
      "  Music: Pearson correlation = 0.427, p-value = 0.33944\n",
      "  Sadness: Pearson correlation = -0.511, p-value = 0.24068\n",
      "  Emotional: Pearson correlation = 0.491, p-value = 0.26270\n",
      "  Affection: Pearson correlation = -0.229, p-value = 0.62088\n",
      "  Anger: Pearson correlation = -0.433, p-value = 0.33174\n",
      "  Negative Emotion: Pearson correlation = -0.506, p-value = 0.24652\n",
      "  Friends: Pearson correlation = -0.489, p-value = 0.26575\n",
      "  Achievement: Pearson correlation = 0.148, p-value = 0.75200\n",
      "  Positive Emotion: Pearson correlation = -0.282, p-value = 0.54005\n",
      "\n",
      "Openness Correlations:\n",
      "  Past Tense Verbs: Pearson correlation = 0.298, p-value = 0.51593\n",
      "  Present Tense Verbs: Pearson correlation = 0.228, p-value = 0.62321\n",
      "  Future Tense Verbs: Pearson correlation = -0.110, p-value = 0.81421\n",
      "  Total Number Words: Pearson correlation = -0.554, p-value = 0.19670\n",
      "  Total Prepositions: Pearson correlation = 0.063, p-value = 0.89396\n",
      "  Total Articles: Pearson correlation = -0.023, p-value = 0.96043\n",
      "  Total Negations: Pearson correlation = 0.061, p-value = 0.89744\n",
      "  First Person Singular: Pearson correlation = -0.380, p-value = 0.39986\n",
      "  First Person Plural: Pearson correlation = -0.127, p-value = 0.78678\n",
      "  First Person: Pearson correlation = -0.431, p-value = 0.33449\n",
      "  Second Person: Pearson correlation = -0.174, p-value = 0.70904\n",
      "  Third Person: Pearson correlation = -0.142, p-value = 0.76116\n",
      "  Total Empathy-related Words: Pearson correlation = -0.504, p-value = 0.24882\n",
      "  Money: Pearson correlation = 0.057, p-value = 0.90318\n",
      "  Work: Pearson correlation = 0.121, p-value = 0.79671\n",
      "  Sleep: Pearson correlation = 0.193, p-value = 0.67800\n",
      "  Occupation: Pearson correlation = 0.350, p-value = 0.44202\n",
      "  Family: Pearson correlation = -0.567, p-value = 0.18454\n",
      "  Swearing Terms: Pearson correlation = -0.254, p-value = 0.58284\n",
      "  Leisure: Pearson correlation = -0.067, p-value = 0.88715\n",
      "  School: Pearson correlation = 0.110, p-value = 0.81371\n",
      "  Optimism: Pearson correlation = -0.088, p-value = 0.85134\n",
      "  Home: Pearson correlation = -0.473, p-value = 0.28431\n",
      "  Sexuality: Pearson correlation = 0.015, p-value = 0.97416\n",
      "  Superhero: Pearson correlation = 0.588, p-value = 0.16490\n",
      "  Religion: Pearson correlation = 0.130, p-value = 0.78176\n",
      "  Body: Pearson correlation = -0.306, p-value = 0.50494\n",
      "  Eating: Pearson correlation = -0.522, p-value = 0.22915\n",
      "  Sports: Pearson correlation = -0.291, p-value = 0.52668\n",
      "  Death: Pearson correlation = -0.396, p-value = 0.37870\n",
      "  Communication: Pearson correlation = 0.301, p-value = 0.51225\n",
      "  Hearing: Pearson correlation = -0.182, p-value = 0.69629\n",
      "  Music: Pearson correlation = -0.391, p-value = 0.38532\n",
      "  Sadness: Pearson correlation = -0.194, p-value = 0.67705\n",
      "  Emotional: Pearson correlation = -0.223, p-value = 0.63128\n",
      "  Affection: Pearson correlation = -0.330, p-value = 0.47033\n",
      "  Anger: Pearson correlation = -0.169, p-value = 0.71719\n",
      "  Negative Emotion: Pearson correlation = -0.366, p-value = 0.41989\n",
      "  Friends: Pearson correlation = -0.350, p-value = 0.44187\n",
      "  Achievement: Pearson correlation = -0.416, p-value = 0.35312\n",
      "  Positive Emotion: Pearson correlation = -0.283, p-value = 0.53865\n",
      "\n",
      "Agreeableness Correlations:\n",
      "  Past Tense Verbs: Pearson correlation = 0.196, p-value = 0.67431\n",
      "  Present Tense Verbs: Pearson correlation = -0.029, p-value = 0.95117\n",
      "  Future Tense Verbs: Pearson correlation = 0.036, p-value = 0.93919\n",
      "  Total Number Words: Pearson correlation = -0.119, p-value = 0.79947\n",
      "  Total Prepositions: Pearson correlation = 0.273, p-value = 0.55378\n",
      "  Total Articles: Pearson correlation = 0.267, p-value = 0.56225\n",
      "  Total Negations: Pearson correlation = -0.322, p-value = 0.48189\n",
      "  First Person Singular: Pearson correlation = -0.595, p-value = 0.15878\n",
      "  First Person Plural: Pearson correlation = -0.014, p-value = 0.97649\n",
      "  First Person: Pearson correlation = -0.449, p-value = 0.31259\n",
      "  Second Person: Pearson correlation = -0.298, p-value = 0.51694\n",
      "  Third Person: Pearson correlation = 0.010, p-value = 0.98365\n",
      "  Total Empathy-related Words: Pearson correlation = -0.631, p-value = 0.12864\n",
      "  Money: Pearson correlation = 0.537, p-value = 0.21416\n",
      "  Work: Pearson correlation = -0.161, p-value = 0.73096\n",
      "  Sleep: Pearson correlation = 0.499, p-value = 0.25432\n",
      "  Occupation: Pearson correlation = 0.586, p-value = 0.16645\n",
      "  Family: Pearson correlation = -0.702, p-value = 0.07893\n",
      "  Swearing Terms: Pearson correlation = -0.429, p-value = 0.33635\n",
      "  Leisure: Pearson correlation = 0.012, p-value = 0.98013\n",
      "  School: Pearson correlation = 0.263, p-value = 0.56882\n",
      "  Optimism: Pearson correlation = -0.065, p-value = 0.88983\n",
      "  Home: Pearson correlation = -0.777, p-value = 0.03971\n",
      "  Sexuality: Pearson correlation = 0.305, p-value = 0.50665\n",
      "  Superhero: Pearson correlation = 0.751, p-value = 0.05189\n",
      "  Religion: Pearson correlation = 0.289, p-value = 0.52947\n",
      "  Body: Pearson correlation = -0.526, p-value = 0.22480\n",
      "  Eating: Pearson correlation = -0.024, p-value = 0.95996\n",
      "  Sports: Pearson correlation = 0.118, p-value = 0.80038\n",
      "  Death: Pearson correlation = -0.248, p-value = 0.59221\n",
      "  Communication: Pearson correlation = 0.515, p-value = 0.23656\n",
      "  Hearing: Pearson correlation = -0.421, p-value = 0.34665\n",
      "  Music: Pearson correlation = -0.254, p-value = 0.58209\n",
      "  Sadness: Pearson correlation = 0.078, p-value = 0.86873\n",
      "  Emotional: Pearson correlation = 0.068, p-value = 0.88492\n",
      "  Affection: Pearson correlation = 0.050, p-value = 0.91570\n",
      "  Anger: Pearson correlation = 0.188, p-value = 0.68710\n",
      "  Negative Emotion: Pearson correlation = -0.507, p-value = 0.24514\n",
      "  Friends: Pearson correlation = -0.493, p-value = 0.26063\n",
      "  Achievement: Pearson correlation = 0.077, p-value = 0.86888\n",
      "  Positive Emotion: Pearson correlation = -0.317, p-value = 0.48834\n",
      "\n",
      "Conscientiousness Correlations:\n",
      "  Past Tense Verbs: Pearson correlation = 0.403, p-value = 0.36954\n",
      "  Present Tense Verbs: Pearson correlation = 0.334, p-value = 0.46472\n",
      "  Future Tense Verbs: Pearson correlation = -0.070, p-value = 0.88197\n",
      "  Total Number Words: Pearson correlation = -0.892, p-value = 0.00695\n",
      "  Total Prepositions: Pearson correlation = -0.630, p-value = 0.12918\n",
      "  Total Articles: Pearson correlation = -0.780, p-value = 0.03863\n",
      "  Total Negations: Pearson correlation = 0.576, p-value = 0.17614\n",
      "  First Person Singular: Pearson correlation = 0.222, p-value = 0.63163\n",
      "  First Person Plural: Pearson correlation = -0.122, p-value = 0.79490\n",
      "  First Person: Pearson correlation = 0.027, p-value = 0.95404\n",
      "  Second Person: Pearson correlation = -0.185, p-value = 0.69111\n",
      "  Third Person: Pearson correlation = 0.218, p-value = 0.63824\n",
      "  Total Empathy-related Words: Pearson correlation = 0.027, p-value = 0.95383\n",
      "  Money: Pearson correlation = -0.193, p-value = 0.67768\n",
      "  Work: Pearson correlation = 0.600, p-value = 0.15449\n",
      "  Sleep: Pearson correlation = -0.480, p-value = 0.27548\n",
      "  Occupation: Pearson correlation = -0.149, p-value = 0.74957\n",
      "  Family: Pearson correlation = -0.187, p-value = 0.68820\n",
      "  Swearing Terms: Pearson correlation = -0.339, p-value = 0.45702\n",
      "  Leisure: Pearson correlation = -0.672, p-value = 0.09846\n",
      "  School: Pearson correlation = -0.600, p-value = 0.15416\n",
      "  Optimism: Pearson correlation = -0.259, p-value = 0.57460\n",
      "  Home: Pearson correlation = 0.168, p-value = 0.71818\n",
      "  Sexuality: Pearson correlation = 0.122, p-value = 0.79443\n",
      "  Superhero: Pearson correlation = -0.022, p-value = 0.96255\n",
      "  Religion: Pearson correlation = -0.281, p-value = 0.54220\n",
      "  Body: Pearson correlation = -0.111, p-value = 0.81206\n",
      "  Eating: Pearson correlation = -0.491, p-value = 0.26351\n",
      "  Sports: Pearson correlation = -0.639, p-value = 0.12236\n",
      "  Death: Pearson correlation = 0.010, p-value = 0.98297\n",
      "  Communication: Pearson correlation = -0.395, p-value = 0.38024\n",
      "  Hearing: Pearson correlation = 0.040, p-value = 0.93154\n",
      "  Music: Pearson correlation = -0.488, p-value = 0.26668\n",
      "  Sadness: Pearson correlation = -0.436, p-value = 0.32797\n",
      "  Emotional: Pearson correlation = -0.170, p-value = 0.71522\n",
      "  Affection: Pearson correlation = -0.377, p-value = 0.40484\n",
      "  Anger: Pearson correlation = -0.381, p-value = 0.39971\n",
      "  Negative Emotion: Pearson correlation = 0.485, p-value = 0.26946\n",
      "  Friends: Pearson correlation = -0.278, p-value = 0.54559\n",
      "  Achievement: Pearson correlation = -0.755, p-value = 0.04990\n",
      "  Positive Emotion: Pearson correlation = -0.205, p-value = 0.65853\n"
     ]
    }
   ],
   "source": [
    "### CORRELATION TEST ###\n",
    "\n",
    "# Language feature data\n",
    "data = {\n",
    "    \"Past Tense Verbs\": [4.04, 4.57, 4.37, 4.32, 3.97, 3.49, 3.60],\n",
    "    \"Present Tense Verbs\": [7.42, 8.74, 9.16, 8.92, 9.14, 7.35, 7.02],\n",
    "    \"Future Tense Verbs\": [1.14, 0.82, 0.81, 0.92, 0.52, 1.13, 1.43],\n",
    "    \"Total Number Words\": [1.00, 0.68, 0.59, 0.69, 0.78, 0.73, 0.65],\n",
    "    \"Total Prepositions\": [6.49, 4.62, 4.98, 5.06, 4.83, 7.29, 5.17], # NOT NORMAL\n",
    "    \"Total Articles\": [6.03, 3.96, 4.25, 4.49, 4.15, 5.82, 3.88],     # NOT NORMAL\n",
    "    \"Total Negations\": [1.29, 1.93, 1.99, 2.24, 1.91, 1.26, 1.66],\n",
    "    \"First Person Singular\": [2.71, 2.98, 2.31, 3.57, 2.57, 2.93, 3.88],\n",
    "    \"First Person Plural\": [4.06, 3.16, 3.85, 3.68, 3.39, 4.56, 5.12],\n",
    "    \"First Person\": [6.78, 6.14, 5.78, 7.25, 5.96, 6.89, 9.00],\n",
    "    \"Second Person\": [3.11, 2.49, 2.57, 3.74, 2.47, 3.96, 3.83],\n",
    "    \"Third Person\": [1.54, 1.84, 1.62, 1.21, 1.97, 1.36, 1.71],\n",
    "    \"Total Empathy-related Words\": [56.49, 54.39, 52.42, 62.08, 53.47, 55.31, 63.30],\n",
    "    \"Money\": [0.57, 0.37, 0.45, 0.17, 0.29, 0.40, 0.51],\n",
    "    \"Work\": [10.97, 15.93, 14.09, 13.87, 15.47, 11.18, 12.56],\n",
    "    \"Sleep\": [0.50, 0.41, 0.42, 0.23, 0.49, 0.53, 0.28],\n",
    "    \"Occupation\": [0.08, 0.04, 0.11, 0.00, 0.07, 0.13, 0.09],\n",
    "    \"Family\": [0.96, 0.63, 0.64, 1.44, 0.74, 0.73, 1.11],\n",
    "    \"Swearing Terms\": [0.32, 0.30, 0.34, 0.52, 0.64, 0.37, 0.14],\n",
    "    \"Leisure\": [0.29, 0.12, 0.22, 0.29, 0.18, 0.27, 0.14],\n",
    "    \"School\": [0.80, 0.23, 0.47, 0.63, 0.18, 0.70, 0.18],\n",
    "    \"Optimism\": [0.48, 0.25, 0.47, 0.52, 0.32, 0.53, 0.55],\n",
    "    \"Home\": [0.65, 0.75, 0.56, 1.27, 0.78, 0.77, 1.20],\n",
    "    \"Sexuality\": [0.10, 0.05, 0.13, 0.00, 0.06, 0.07, 0.18],\n",
    "    \"Superhero\": [0.08, 0.05, 0.11, 0.06, 0.04, 0.07, 0.05],\n",
    "    \"Religion\": [0.50, 0.41, 0.20, 0.35, 0.10, 0.40, 0.23],\n",
    "    \"Body\": [0.86, 0.62, 0.72, 1.38, 0.68, 1.03, 1.15],\n",
    "    \"Eating\": [1.27, 0.60, 0.70, 0.52, 0.76, 0.67, 1.15],\n",
    "    \"Sports\": [1.16, 0.64, 0.47, 0.63, 0.46, 0.53, 0.46],          # NOT NORMAL\n",
    "    \"Death\": [1.05, 0.77, 0.50, 0.75, 0.52, 0.83, 1.75],\n",
    "    \"Communication\": [1.47, 1.20, 1.11, 1.15, 0.83, 1.36, 0.92],\n",
    "    \"Hearing\": [1.24, 1.17, 1.22, 2.13, 1.04, 0.83, 1.02],         # NOT NORMAL\n",
    "    \"Music\": [1.56, 1.14, 1.16, 1.55, 1.09, 1.10, 1.15],           # NOT NORMAL\n",
    "    \"Sadness\": [0.53, 0.11, 0.21, 0.17, 0.18, 0.60, 0.60],         # NOT NORMAL\n",
    "    \"Emotional\": [0.49, 0.40, 0.48, 0.40, 0.45, 0.27, 0.37],\n",
    "    \"Affection\": [0.20, 0.07, 0.14, 0.12, 0.08, 0.10, 0.18],\n",
    "    \"Anger\": [0.31, 0.05, 0.22, 0.12, 0.10, 0.23, 0.32],\n",
    "    \"Negative Emotion\": [1.58, 1.87, 1.88, 2.07, 1.91, 1.23, 2.40],\n",
    "    \"Friends\": [1.18, 0.81, 0.98, 1.55, 1.01, 1.40, 1.43],\n",
    "    \"Achievement\": [1.14, 0.26, 0.31, 0.35, 0.31, 0.50, 0.55],     # NOT NORMAL\n",
    "    \"Positive Emotion\": [1.04, 0.62, 0.88, 1.27, 0.63, 0.90, 1.11],\n",
    "}\n",
    "\n",
    "\n",
    "# Big Five personality scores\n",
    "big_five_data = {\n",
    "    \"Neuroticism\": [3.22, 4.22, 5.52, 3.0, 3.15, 3.85, 4.43],\n",
    "    \"Extroversion\": [4.9, 4.65, 4.36, 4.36, 4.23, 3.92, 2.65],\n",
    "    \"Openness\": [4.02, 5.12, 5.52, 4.27, 3.86, 5.13, 4.08],\n",
    "    \"Agreeableness\": [3.76, 4.07, 5.07, 1.95, 2.15, 4.11, 2.6],\n",
    "    \"Conscientiousness\": [3.01, 6.22, 5.73, 4.88, 4.16, 4.36, 5.49],\n",
    "}\n",
    "\n",
    "# Correlation results storage\n",
    "correlation_results = {}\n",
    "\n",
    "# Perform correlation tests\n",
    "for trait, scores in big_five_data.items():\n",
    "    correlation_results[trait] = {}\n",
    "    for label, values in data.items():\n",
    "        # Use Spearman for non-normal distributions, otherwise use Pearson\n",
    "        if label in [\"Total prepositions\", \"sports\"]:\n",
    "            corr, p_value = spearmanr(scores, values)\n",
    "            test_type = \"Spearman\"\n",
    "        else:\n",
    "            corr, p_value = pearsonr(scores, values)\n",
    "            test_type = \"Pearson\"\n",
    "        \n",
    "        # Store results in dictionary\n",
    "        correlation_results[trait][label] = (test_type, corr, p_value)\n",
    "\n",
    "# Print correlation results\n",
    "for trait, results in correlation_results.items():\n",
    "    print(f\"\\n{trait} Correlations:\")\n",
    "    for label, (test_type, corr, p_value) in results.items():\n",
    "        print(f\"  {label}: {test_type} correlation = {corr:.3f}, p-value = {p_value:.5f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13a73f8-31f9-4eb2-8d53-aa0352b91e24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Statistical Results on Correlation Between Language Use and Personality\n",
    "\n",
    "After running statistical tests, most correlations did not reach significance (p > 0.05). This may be attributed to the limited sample size of seven characters, compared to the larger datasets in reference studies. So instead of focusing on this statistical results, we will manually analyzing character-language use relationships in the Disccusion section later.\n",
    "\n",
    "**Significant Correlations** <br>\n",
    "\n",
    "- `Number` words are negatively correlated with conscientiousness, consistent with previous findings and our hypothesis.\n",
    "- Unexpectedly, `articles` and `achievement` words show a negative association with conscientiousness, contradicting our hypothesis.\n",
    "- Contrary to expectations, `first person singular and plural pronouns` do not positively correlate with extraversion.\n",
    "- The `family` and `home` lexicons align with prior results, showing a negative association, supporting our hypothesis.\n",
    "\n",
    "\n",
    "|                           | Neuroticism                | Extroversion                | Openness                   | Agreeableness              | Conscientiousness          |\n",
    "|---------------------------|----------------------------|-----------------------------|----------------------------|----------------------------|----------------------------|\n",
    "|**Tense Verbs**|||||\n",
    "| Past Tense Verbs          | 0.138                      | 0.666                       | 0.298                      | 0.196                      | 0.403                      |\n",
    "| Present Tense Verbs       | 0.063                      | 0.503                       | 0.228                      | -0.029                     | 0.334                      |\n",
    "| Future Tense Verbs        | 0.123                      | -0.584                      | -0.110                     | 0.036                      | -0.070                     |\n",
    "|**Numbers**|||||\n",
    "| Total Number Words        | -0.658                     | 0.453                       | -0.554                     | -0.119                     | **-0.892**                     |\n",
    "|**Prepositions**|||||\n",
    "| Total Prepositions         | -0.216                     | 0.030                       | 0.063                      | 0.273                      | -0.630                     |\n",
    "|**Articles**|||||\n",
    "| Total Articles            | -0.365                     | 0.362                       | -0.023                     | 0.267                      | **-0.780**                     |\n",
    "|**Negations**|||||\n",
    "| Total Negations           | 0.088                      | 0.098                       | 0.061                      | -0.322                     | 0.576                      |\n",
    "|**Pronouns**|||||\n",
    "| First Person Singular     | -0.216                     | -0.653                      | -0.380                     | -0.595                     | 0.222                      |\n",
    "| First Person Plural       | 0.208                      | **-0.782**                  | -0.127                     | -0.014                     | -0.122                     |\n",
    "| First Person              | -0.079                     | **-0.800**                  | -0.431                     | -0.449                     | 0.027                      |\n",
    "| Second Person             | -0.228                     | -0.550                      | -0.174                     | -0.298                     | -0.185                     |\n",
    "| Third Person              | 0.206                      | -0.072                      | -0.142                     | 0.010                      | 0.218                      |\n",
    "|**Empathy**|||||\n",
    "| Total Empathy-related Words| -0.279                    | -0.585                      | -0.504                     | -0.631                     | 0.027                      |\n",
    "| Money                     | 0.418                      | -0.167                      | 0.057                      | 0.537                      | -0.193                     |\n",
    "| Work                      | 0.120                      | 0.189                       | 0.121                      | -0.161                     | 0.600                      |\n",
    "| Sleep                     | -0.029                     | 0.410                       | 0.193                      | 0.499                      | -0.480                     |\n",
    "| Occupation                | 0.505                      | -0.302                      | 0.350                      | 0.586                      | -0.149                     |\n",
    "| Family                    | -0.494                     | -0.244                      | -0.567                     | **-0.702**                 | -0.187                     |\n",
    "| Swearing Terms            | -0.571                     | 0.451                       | -0.254                     | -0.429                     | -0.339                     |\n",
    "| Leisure                   | -0.411                     | 0.394                       | -0.067                     | 0.012                      | -0.672                     |\n",
    "| School                    | -0.277                     | 0.453                       | 0.110                      | 0.263                      | -0.600                     |\n",
    "| Optimism                  | 0.051                      | -0.470                      | -0.088                     | -0.065                     | -0.259                     |\n",
    "| Home                      | -0.342                     | -0.574                      | -0.473                     | **-0.777**                     | 0.168                      |\n",
    "| Sexuality                 | 0.627                      | -0.606                      | 0.015                      | 0.305                      | 0.122                      |\n",
    "| Religion                  | -0.266                     | 0.422                       | 0.130                      | 0.289                      | -0.281                     |\n",
    "| Body                      | -0.330                     | -0.432                      | -0.306                     | -0.526                     | -0.111                     |\n",
    "| Eating                    | -0.047                     | -0.260                      | -0.522                     | -0.024                     | -0.491                     |\n",
    "| Sports                    | -0.426                     | 0.578                       | -0.291                     | 0.118                      | -0.639                     |\n",
    "| Death                     | 0.037                      | -0.725                      | -0.396                     | -0.248                     | 0.010                      |\n",
    "| Communication             | -0.130                     | 0.533                       | 0.301                      | 0.515                      | -0.395                     |\n",
    "| Hearing                   | -0.355                     | 0.302                       | -0.182                     | -0.421                     | 0.040                      |\n",
    "| Music                     | -0.519                     | 0.427                       | -0.391                     | -0.254                     | -0.488                     |\n",
    "| Sadness                   | -0.013                     | -0.511                      | -0.194                     | 0.078                      | -0.436                     |\n",
    "| Emotional                 | 0.051                      | 0.491                       | -0.223                     | 0.068                      | -0.170                     |\n",
    "| Affection                 | 0.091                      | -0.229                      | -0.330                     | 0.050                      | -0.377                     |\n",
    "| Anger                     | 0.211                      | -0.433                      | -0.169                     | 0.188                      | -0.381                     |\n",
    "| Negative Emotion          | 0.160                      | -0.506                      | -0.366                     | -0.507                     | 0.485                      |\n",
    "| Friends                   | -0.357                     | -0.489                      | -0.350                     | -0.493                     | -0.278                     |\n",
    "| Achievement               | -0.312                     | 0.148                       | -0.416                     | 0.077                      | **-0.755**                     |\n",
    "| Positive Emotion          | -0.183                     | -0.282                      | -0.283                     | -0.317                     | -0.205                     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42a6d3-7427-42bb-89dd-dca3d54dc170",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "In examining the language use patterns by character, our goal was to validate hypothesized the relationships between linguistic markers and the Big Five personality traits proven in Yarkoni (2011). Despite some observed trends, most correlations were not statistically significant, likely due to the limited sample size of only seven characters.\n",
    "\n",
    "For the mannual inspection, we look back at the **Results on Language Use per Character**. <br>\n",
    "\n",
    "#### Neuroticism\n",
    "Albus Dumbledore scored the highest in Neuroticism, which suggested that he would use more first-person singular pronouns, negations, and language expressing negative emotions. However, the data revealed inconsistencies. Lord Voldemort led in first-person singular pronouns, contrary to expectations. Similarly, negations, which should correlate positively with Neuroticism, showed no significant difference between Dumbledore (1.29%) and Severus Snape (1.26%), who unexpectedly had lower-than-anticipated negation use.\n",
    "\n",
    "#### Extraversion\n",
    "Ron Weasley, with the highest Extraversion score (4.9), was expected to use more social and affective language, including plural first-person pronouns, second-person pronouns, and positive-emotion words. Yet these associations did not hold. Notably, Snape, the lowest in Extraversion (2.65), used second-person pronouns most frequently (5.49%), while Ron's use was lowest at 3.27%, a reversal of the hypothesized pattern. In addition, lexical categories like “family” and “communication,” thought to align with extraversion, did not show consistent results.\n",
    "\n",
    "#### Openness\n",
    "Dumbledore’s high score in Openness (5.52) suggested a tendency toward language expressing abstract or intellectual engagement, such as increased present-tense verbs, prepositions, and positive emotion words. Yet, neither Dumbledore nor Ron, the lowest scorer in Openness (4.02), showed language patterns that strongly supported this trend. Prepositions and positive emotion words did not align meaningfully with Openness, with only minimal correlation observed for present-tense verbs (r = 0.228).\n",
    "\n",
    "#### Agreeableness\n",
    "Harry Potter, with a high Agreeableness score (4.11), was hypothesized to use more positive and social language (e.g., family, friends, and past-tense verbs). However, correlations for these categories remained inconsistent. For instance, negative emotion words, expected to be lower among highly agreeable characters, showed mixed results, with Voldemort (1.95 Agreeableness) showing unexpectedly low negative emotion use compared to others.\n",
    "\n",
    "#### Conscientiousness\n",
    "Hermione Granger, with the highest Conscientiousness score (6.22), was expected to avoid negations and negative emotions while using more language related to achievement and work. Although a negative association was found between conscientiousness and numbers (r = -0.892), supporting prior research, other categories like achievement showed only weak associations, and negations were inconsistently distributed across characters.\n",
    "\n",
    "### Conclusion\n",
    "Based on the current dataset, we were unable to confirm the hypothesized correlations between the characters’ personality dimensions and their linguistic patterns. This may largely be due to the small sample size, with only seven characters analyzed, which limits statistical power and makes individual differences more influential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97124465-15e6-4dcd-934d-237001c17d89",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Reference\n",
    "- Basto, C. (2021). Extending the abstraction of personality types based on MBTI with machine learning and natural language processing. _arXiv preprint arXiv:2105.11798._\n",
    "- Ginting, S. A. (2018). Syntactic complexity on extraverted and introverted Indonesian langugae learners' written products. _International Journal of Education and Literacy Studies, 6_(4), 101-106.\n",
    "- John, O. P., Donahue, E. M., \\& Kentle, R. L. (1991). Big five inventory. _Journal of personality and social psychology_.\n",
    "- Pennebaker J. W., Francis M. E., Booth R. J. (2001). Linguistic Inquiry and Word Count (LIWC): LIWC 2001. _Mahwah, NJ: Erlbaum_.\n",
    "- Tausczik, Y. R., \\& Pennebaker, J. W. (2010). The Psychological Meaning of Words: LIWC and Computerized Text Analysis Methods. _Journal of Language and Social Psychology, 29_(1), 24-54.\n",
    "- Yarkoni, T. (2010). Personality in 100,000 words: A large-scale analysis of personality and word use among bloggers. _Journal of research in personality, 44_(3), 363-373.\n",
    "\n",
    "_© Yifei Chen, Chi Kuan Lai. 2024_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
